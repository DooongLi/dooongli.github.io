<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Dong Li</title>

    <meta name="author" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon"
        href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåç</text></svg>">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Dong Li</name>
                                    </p>
                                    <p>I am a junior researcher at <a href="https://www.shlab.org.cn/">Shanghai AI
                                            Lab.</a>, where I work on multimodal machine learning and large language
                                        model.
                                    </p>
                                    <p>
                                        <!-- At Google I've worked on <a href="https://www.google.com/glass/start/">Glass</a>,  <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://blog.google/products/google-ar-vr/introducing-next-generation-jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, <a href="https://ai.googleblog.com/2020/12/portrait-light-enhancing-portrait.html">Portrait Light</a>, and <a href="https://www.matthewtancik.com/nerf">NeRF</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>. -->
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:liddalidd@gmail.com">Email</a> &nbsp/&nbsp
                                        <a href="">CV</a> &nbsp/&nbsp
                                        <a href="https://scholar.google.com/citations?user=bxmsqZIAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                                        <a href="https://github.com/DooongLi">Github</a>
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/haoran_rec.png"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/haoran_rec.png" class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        I'm interested in Multimodal Machine Learning (MML), large language models
                                        (LLMs), efficient Transformers and optimization. Much of my research is about
                                        MML and LLM . Representative papers are <span
                                            class="highlight">highlighted</span>.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <!-- FAVD -->
                            <tr bgcolor="#ffffd0">
                                <td style="padding:20px;width:35%;vertical-align:middle">
                                    <img style="width:100%;max-width:100%" src='images/favd_c.png'>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="http://www.avlbench.opennlplab.cn/papers/favd">
                                        <papertitle>Fine-grained Audible Video Description</papertitle>
                                    </a>
                                    <br>
                                    Xuyang Shen*,
                                    <strong>Dong Li*</strong>,
                                    Jinxing Zhou*, Zhen Qin, Bowen He, Xiaodong Han, Aixuan Li, Yuchao Dai, Lingpeng
                                    Kong, Meng Wang, Yu Qiao, Yiran Zhong
                                    <br>
                                    <em>CVPR</em>, 2023
                                    <br>
                                    <a href="http://www.avlbench.opennlplab.cn/papers/favd">project page</a>
                                    /
                                    <a href="https://youtu.be/iWJvTB-bTWk">video</a>
                                    /
                                    <a href="https://arxiv.org/abs/2303.15616">arXiv</a>
                                    <p></p>
                                    <p style="font-size:small;">
                                        We explore a new task for audio-visual-language modeling called fine-grained
                                        audible video description (FAVD). It aims to provide detailed textual
                                        descriptions for the given audible videos, including the appearance and spatial
                                        locations of each object, the actions of moving objects, and the sounds in
                                        videos.
                                    </p>
                                </td>
                            </tr>

                            <!-- TNN -->
                            <tr>
                                <td style="padding:20px;width:35%;vertical-align:middle">
                                    <img style="width:100%;max-width:100%" src='images/TNN.png'>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://openreview.net/pdf?id=IxmWsm4xrua">
                                        <papertitle>Toeplitz Neural Network for Sequence Modeling</papertitle>
                                    </a>
                                    <br>
                                    
                                    Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, <strong>Dong Li</strong>, Dongxu Li, Yuchao Dai, Lingpeng Kong, Yiran Zhong
                                    <br>
                                    <em>ICLR</em>, 2023 <font color="red"><strong>(notable-top-25%)</strong></font>
                                    <br>
                                    <a href="https://openreview.net/pdf?id=IxmWsm4xrua">arXiv</a>
                                    /
                                    <a href="https://github.com/OpenNLPLab/Tnn">code</a>
                                    <p></p>
                                    <p style="font-size:small;">
                                        We propose to model sequences with a relative position encoded Toeplitz matrix and use a Toeplitz matrix-vector production trick to reduce the space-time complexity of the sequence modeling to log linear. 
                                    </p>
                                </td>
                            </tr>

                            <!-- NAS -->
                            <tr >
                                <td style="padding:20px;width:35%;vertical-align:middle">
                                        <img style="width:100%;max-width:100%" src='images/NAS.png'>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/2207.13955.pdf">
                                        <papertitle>Neural Architecture Search on Efficient Transformers and Beyond</papertitle>
                                    </a>        

                                    <br>
                                    Zexiang Liu*, <strong>Dong Li*</strong>, Kaiyue Lu*, Zhen Qin, Weixuan Sun, Jiacheng Xu, Yiran Zhong
                                    <br>
                                    <em>arXiv</em>, 2022
                                    <br>
                                    <a href="https://arxiv.org/pdf/2207.13955.pdf">arXiv</a>
                                    <p></p>
                                    <p style="font-size:small;">
                                        We propose a new framework to find optimal architectures for efficient Transformers with the neural architecture search (NAS) technique. 
                                    </p>
                                </td>
                            </tr>

                            <!-- FAVD -->
                            <tr >
                                <td style="padding:20px;width:35%;vertical-align:middle">
                                    <img style="width:100%;max-width:100%" src='images/linear-video.png'>
                                </td>
                                <td style="padding:20px;width:75%;vertical-align:middle">
                                    <a href="https://arxiv.org/pdf/2210.08164.pdf">
                                        <papertitle>Linear Video Transformer with Feature Fixation</papertitle>
                                    </a>
                                    <br>
                                    Kaiyue Lu, Zexiang Liu, Jianyuan Wang, Weixuan Sun, Zhen Qin, <strong>Dong Li</strong>, Xuyang Shen, Hui Deng, Xiaodong Han, Yuchao Dai, Yiran Zhong
                                    <br>
                                    <em>arXiv</em>, 2022
                                    <br>
                                    <a href="https://arxiv.org/pdf/2210.08164.pdf">arXiv</a>
                                    <p></p>
                                    <p style="font-size:small;">
                                        We propose a feature fixation module to reweight the feature importance of the query and key before computing linear attention. 
                                    </p>
                                </td>
                            </tr>
                            
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        Feel free to steal this website's <a
                                            href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                                        <strong>Do not</strong> scrape the HTML from this page itself, as it includes
                                        analytics tags that you do not want on your own website &mdash; use the github
                                        code instead. Also, consider using <a href="https://leonidk.com/">Leonid
                                            Keselman</a>'s <a href="https://github.com/leonidk/new_website">Jekyll
                                            fork</a> of this page.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>
</body>

</html>